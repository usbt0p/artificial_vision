================================================================================
  LECTURE 13: VISUAL POLICY LEARNING - COMPLETE DEMO SERIES
  From Imitation to Foundation Models
  
  Prof. David Olivieri - UVigo - VIAR25/26
  Artificial Vision Course (VIAR25/26)
================================================================================

OVERVIEW
========

This collection contains 10 comprehensive PyTorch demonstrations covering the
evolution of visual policy learning methods, from basic imitation learning to
modern foundation models.

Each demo is:
  â€¢ Self-contained and runnable independently
  â€¢ Extensively commented for educational clarity
  â€¢ Includes comprehensive visualizations
  â€¢ References original research papers
  â€¢ Follows consistent coding patterns

DEMO SERIES STRUCTURE
======================

The demos follow a progressive learning arc:

Part 1: Core Visual RL Methods (Demos 1-4)
  - Foundational approaches to learning from pixels
  - Direct policy learning and value-based methods

Part 2: Advanced Imitation Learning (Demos 5-6)
  - Beyond basic behavioral cloning
  - Adversarial and interactive approaches

Part 3: Representation Learning & Robustness (Demos 7-9)
  - Learning better visual features
  - Improving sample efficiency and transfer

Part 4: Foundation Models (Demo 10)
  - Scaling to internet-scale pretraining
  - Vision-language-action transformers

================================================================================

DEMO 1: BEHAVIORAL CLONING (behavioral_cloning.py)
===================================================

OVERVIEW
--------
Demonstrates supervised learning approach to imitation learning. The simplest
form of learning from demonstrations: collect expert data, train policy to
predict expert actions given states.

KEY FEATURES
------------
1. Expert Policy
   - Heuristic policy for CartPole demonstrations
   - Collects state-action pairs for training

2. BC Policy Network
   - MLP architecture (state â†’ action)
   - Trained via supervised learning (cross-entropy loss)

3. Expert Data Collector
   - Gathers demonstrations (100 episodes default)
   - Creates training/validation splits

4. BC Trainer
   - Cross-entropy loss minimization
   - Adam optimizer with learning rate scheduling
   - Early stopping based on validation performance

5. Policy Evaluator
   - Compares Expert vs BC vs Random
   - Statistical performance analysis

6. Distribution Shift Analyzer
   - Demonstrates covariate shift problem
   - Shows how BC policies drift from expert distribution
   - Visualizes compounding errors over time

VISUALIZATIONS
--------------
â€¢ Training curves (loss, accuracy)
â€¢ Performance comparison (bar plots)
â€¢ State distribution analysis
â€¢ Distribution shift demonstration

KEY INSIGHT
-----------
BC learns reasonable policies but suffers from distribution shift:
  Training:  Expert visits states S_expert
  Testing:   Policy visits states S_policy
  Problem:   S_expert â‰  S_policy â†’ compounding errors

EXPECTED RESULTS (CartPole)
---------------------------
  Expert:  200 Â± 50
  BC:      150 Â± 70  (75% of expert performance)
  Random:   20 Â± 10

Distribution shift causes ~25% performance degradation

LIMITATIONS
-----------
â€¢ Sensitive to expert data quality
â€¢ No correction mechanism for errors
â€¢ Compounding errors over time
â€¢ Poor generalization outside training distribution

================================================================================

DEMO 2: PPO VISUAL CONTROL (PPO_visual_control.py)
===================================================

OVERVIEW
--------
Proximal Policy Optimization for visual control. On-policy policy gradient
method with clipped surrogate objective and visual observations.

KEY FEATURES
------------
1. Visual Wrapper
   - Converts environment to 84Ã—84 pixel observations
   - RGB rendering from any Gym environment

2. CNN Encoder
   - Nature DQN architecture
   - 3 convolutional layers (32â†’64â†’64 filters)
   - Processes visual observations to features

3. Actor-Critic Network
   - Shared CNN encoder
   - Separate actor and critic heads
   - Outputs action probabilities and value estimates

4. Rollout Buffer
   - Stores on-policy trajectories (2048 steps default)
   - Computes GAE (Generalized Advantage Estimation)
   - Provides training batches

5. PPO Trainer
   - Clipped surrogate objective: min(ratio Ã— A, clip(ratio, 1-Îµ, 1+Îµ) Ã— A)
   - Value function clipping for stability
   - Entropy bonus for exploration
   - Multiple epochs per rollout (4 default)

6. PPO Agent
   - Collects rollouts using current policy
   - Trains on collected data
   - Tracks training statistics

VISUALIZATIONS
--------------
6 training metrics plots:
  1. Episode returns
  2. Policy loss
  3. Value loss
  4. Entropy (exploration measure)
  5. KL divergence (policy change)
  6. Clip fraction (constraint activity)

KEY MECHANISMS
--------------
â€¢ Trust Region via Clipping: Prevents large policy updates
â€¢ GAE: Reduces variance in advantage estimates
â€¢ On-policy Learning: Updates from current policy's data
â€¢ Value Function Baseline: Reduces gradient variance

HYPERPARAMETERS
---------------
  CLIP_EPSILON = 0.2      # PPO clipping parameter
  GAE_LAMBDA = 0.95       # GAE smoothing
  NUM_EPOCHS = 4          # Optimization epochs per rollout
  BATCH_SIZE = 256        # Mini-batch size
  LEARNING_RATE = 3e-4    # Adam learning rate

EXPECTED RESULTS (CartPole)
---------------------------
  â€¢ Converges in ~50k steps
  â€¢ Final performance: 190-200 return
  â€¢ Stable learning (low variance)

PPO ADVANTAGES
--------------
â€¢ Robust and stable training
â€¢ Simple to implement
â€¢ Works across many domains
â€¢ Good sample efficiency for on-policy

================================================================================

DEMO 3: SAC PIXELS (SAC_pixels.py)
===================================

OVERVIEW
--------
Soft Actor-Critic for learning from pixels. Off-policy maximum entropy RL
algorithm with continuous action spaces (adapted for discrete actions).

KEY FEATURES
------------
1. CNN Encoder
   - Same architecture as PPO
   - Layer normalization for stability
   - Shared between Q-networks and policy

2. Twin Q-Networks
   - Two Q-networks (Q1, Q2) to reduce overestimation
   - Takes minimum for target computation
   - Separate optimizers

3. SAC Policy
   - Stochastic policy with temperature parameter
   - Outputs action probabilities (discrete adaptation)
   - Entropy-regularized objective

4. Replay Buffer
   - Off-policy storage (100k capacity)
   - Uniform sampling for training
   - Efficient memory management

5. SAC Trainer
   - Soft Q-targets: Q(s,a) - Î±Â·log Ï€(a|s)
   - Policy loss: Maximize E[Q(s,a) - Î±Â·log Ï€(a|s)]
   - Automatic entropy tuning (learnable temperature Î±)
   - Target networks with soft updates (Ï„=0.005)

6. SAC Agent
   - Warmup phase (random actions)
   - Off-policy updates every step
   - Epsilon-greedy exploration (decaying)

VISUALIZATIONS
--------------
5 training metrics:
  1. Episode returns
  2. Q-network losses (Q1, Q2)
  3. Policy loss
  4. Temperature (Î±) schedule
  5. Policy entropy

KEY DIFFERENCES FROM PPO
------------------------
  PPO:  On-policy, policy gradients, clipped objective
  SAC:  Off-policy, Q-learning + policy, maximum entropy

HYPERPARAMETERS
---------------
  GAMMA = 0.99                    # Discount factor
  TAU = 0.005                     # Soft update rate
  TARGET_ENTROPY = -log(|A|)Ã—0.98 # Entropy target
  BUFFER_SIZE = 100000            # Replay capacity
  BATCH_SIZE = 256                # Training batch size

EXPECTED RESULTS (CartPole)
---------------------------
  â€¢ More sample efficient than PPO
  â€¢ Better exploration via entropy maximization
  â€¢ Final performance: 180-200 return
  â€¢ ~33% improvement over vanilla DQN

SAC ADVANTAGES
--------------
â€¢ Off-policy: Better sample efficiency
â€¢ Maximum entropy: Better exploration
â€¢ Twin Q-networks: Reduced overestimation
â€¢ Automatic temperature tuning: Adaptive exploration
â€¢ Robust across tasks

================================================================================

DEMO 4: DrQ AUGMENTATION (DrQ_augmentation.py)
===============================================

OVERVIEW
--------
Data-Regularized Q-learning with image augmentation. Shows that simple random
crops dramatically improve sample efficiency in visual RL.

KEY FEATURES
------------
1. Image Augmentation Module
   Four augmentation strategies:
   
   â€¢ random_crop: Pad by 4 pixels, random crop to 84Ã—84 (KEY METHOD)
     - Most effective augmentation for visual RL
     - Provides translation invariance
     - Zero additional parameters
   
   â€¢ random_shift: Translation augmentation
     - Similar to crop but different implementation
   
   â€¢ random_intensity: Brightness variation
     - Color/lighting invariance
   
   â€¢ no_augmentation: Baseline (identity)

2. DrQ Network
   - Q-network with CNN encoder
   - Standard DQN architecture
   - No special modifications needed

3. DrQ Trainer
   - Multiple augmented views per sample (default: 2)
   - Average loss over all views
   - Epsilon-greedy with decay
   - Target network soft updates

4. Ablation Study
   - Compares DrQ (Crop) vs No Augmentation vs Random
   - Fair comparison (same hyperparameters)
   - Statistical evaluation

VISUALIZATIONS
--------------
â€¢ Augmentation examples (side-by-side comparison)
â€¢ Training curves (4 metrics)
â€¢ Performance comparison (bar plots)
â€¢ Sample efficiency analysis

KEY INSIGHT
-----------
Data augmentation = Implicit regularization

Training on augmented views:
  1. Creates effective data multiplication
  2. Enforces spatial invariance
  3. Prevents overfitting to specific pixels
  4. Improves generalization

EXPECTED RESULTS (CartPole)
---------------------------
  No Aug:      150 Â± 70  (baseline)
  DrQ (Crop):  200 Â± 50  (~33% improvement!)
  Random:       20 Â± 10

Same training time, zero additional parameters, major gains!

WHY IT WORKS
------------
Random crops teach the policy to be invariant to:
  â€¢ Small translations
  â€¢ Exact pixel positions
  â€¢ Minor visual variations

This creates a more robust visual representation.

DrQ ADVANTAGES
--------------
â€¢ Simple to implement (just add augmentation!)
â€¢ Zero additional model parameters
â€¢ Major sample efficiency gains
â€¢ Works with any Q-learning algorithm
â€¢ Minimal computational overhead

BEST PRACTICE
--------------
Always use random crop augmentation for visual RL!
It's the easiest performance boost available.

================================================================================

DEMO 5: GAIL IMPLEMENTATION (GAIL_implementation.py)
====================================================

OVERVIEW
--------
Generative Adversarial Imitation Learning. Uses discriminator to distinguish
expert from learner, training policy to fool the discriminator.

KEY FEATURES
------------
1. Discriminator Network
   - Binary classifier (expert vs. learner)
   - Takes state-action pairs as input
   - Outputs probability D(s,a) âˆˆ [0,1]
     * D(s,a) â‰ˆ 1: Expert-like
     * D(s,a) â‰ˆ 0: Learner-like
   - MLP with dropout for stability

2. GAIL Policy Network
   - Generator/learner policy
   - Standard MLP architecture
   - Trained to maximize "expertness"

3. Adversarial Training Framework
   
   Discriminator Update:
   - Binary cross-entropy loss
   - Maximize accuracy on expert data (label = 1)
   - Maximize accuracy on learner data (label = 0)
   
   Policy Update:
   - Policy gradient with discriminator rewards
   - Reward: r(s,a) = log D(s,a)
   - Maximizes "fooling" the discriminator
   - GAE for advantage estimation

4. GAIL Buffer
   - Stores learner trajectories
   - On-policy collection (cleared each iteration)
   - Provides data for both discriminator and policy

5. Complete Training Loop
   ```
   For each iteration:
     1. Collect trajectories with current policy
     2. Update discriminator (expert vs. learner)
     3. Update policy using discriminator rewards
     4. Repeat
   ```

VISUALIZATIONS
--------------
6 training curves:
  1. Episode returns over iterations
  2. Discriminator loss
  3. Policy loss
  4. Expert accuracy (discriminator on expert data)
  5. Learner accuracy (discriminator on learner data)
  6. Combined accuracy plot

GAIL VS BC COMPARISON
---------------------
| Feature            | BC              | GAIL                  |
|--------------------|-----------------|------------------------|
| Learning           | Supervised      | Adversarial RL        |
| Training           | One-shot        | Iterative             |
| Objective          | Match actions   | Match distributions   |
| Distribution Shift | Suffers         | Robust                |
| Sample Efficiency  | Lower           | Higher                |
| Complexity         | Simple          | Moderate              |

EXPECTED RESULTS (CartPole)
---------------------------
  Expert:  200 Â± 50
  GAIL:    180 Â± 60  (90% of expert)
  BC:      150 Â± 70  (75% of expert)
  Random:   20 Â± 10

GAIL improvement over BC: ~20%

KEY ADVANTAGE
-------------
Unlike BC which learns:
  Ï€(a|s) â‰ˆ Ï€_expert(a|s)  [individual pairs]

GAIL learns:
  p_Ï€(s,a) â‰ˆ p_expert(s,a)  [joint distribution]

This distribution matching is more robust to compounding errors!

TRAINING DYNAMICS
-----------------
Early Training:
  - Discriminator easily distinguishes expert from learner
  - Expert accuracy: ~90%, Learner accuracy: ~90%
  - Policy receives strong signal to improve

Mid Training:
  - Policy gets better, discriminator task harder
  - Accuracies converge toward 50%
  - Indicates distribution matching

Late Training:
  - Both converge to equilibrium
  - Policy closely mimics expert distribution
  - Discriminator struggles (both look similar)

RESEARCH IMPACT
---------------
GAIL showed that:
  â€¢ Imitation learning can be framed as adversarial training
  â€¢ Distribution matching > direct action matching
  â€¢ No explicit reward function needed
  â€¢ Scales to high-dimensional state spaces

================================================================================

DEMO 6: DAgger IMPLEMENTATION (DAgger_demo.py)
===============================================

OVERVIEW
--------
Dataset Aggregation for interactive imitation learning. Addresses BC's
distribution shift problem through expert queries on learner-visited states.

KEY FEATURES
------------
1. Interactive Expert Oracle
   - Can be queried for optimal actions
   - Tracks number of expert queries (important metric)
   - Demonstrates oracle-based learning

2. DAgger Policy Network
   - Same architecture as BC policy
   - Shows that algorithm matters, not architecture

3. Aggregated Dataset
   - Stores data from all iterations
   - Tracks contribution from each iteration
   - Grows over time as new data is added
   - Key difference from BC (single dataset)

4. DAgger Algorithm Implementation
   ```
   Initial: Collect Dâ‚€ from expert demonstrations
   
   For iteration i = 1 to N:
     1. Train policy Ï€áµ¢ on aggregated dataset Dáµ¢â‚‹â‚
     2. Run Ï€áµ¢ to collect states Sáµ¢
     3. Query expert for actions on Sáµ¢ â†’ get labels Aáµ¢
     4. Aggregate: Dáµ¢ = Dáµ¢â‚‹â‚ âˆª {Sáµ¢, Aáµ¢}
   ```

5. Beta Scheduling
   Three strategies for mixing expert/learner during rollouts:
   
   â€¢ Constant (Î²=1.0): Always follow expert during collection
   â€¢ Linear decay: Î² = 1 - i/N (gradually reduce expert)
   â€¢ Exponential decay: Î² = 0.5^i (aggressive reduction)

6. On-Policy Data Collection (Core DAgger Innovation)
   - Runs current policy to visit states
   - Queries expert for labels on visited states
   - Creates dataset from learner's state distribution

VISUALIZATIONS
--------------
6 training curves:
  1. Policy Performance (return over iterations)
  2. Training Accuracy (how well policy fits dataset)
  3. Dataset Size (growth of aggregated data)
  4. Training Loss (convergence monitoring)
  5. Expert Queries (cost of oracle access)
  6. Beta Schedule (exploration strategy)

Plus:
  â€¢ Performance comparison (DAgger vs BC vs GAIL vs Expert vs Random)
  â€¢ Learning curve comparison
  â€¢ Distribution analysis

THE DISTRIBUTION SHIFT PROBLEM (BC)
-----------------------------------
Training:     Expert visits states S_expert
Testing:      Policy visits states S_policy
Problem:      S_expert â‰  S_policy â†’ poor performance

DAGGER'S SOLUTION
-----------------
Iteration 1:  Policy visits Sâ‚ â†’ Expert labels Sâ‚
Iteration 2:  Policy visits Sâ‚‚ â†’ Expert labels Sâ‚‚
...
Result:       Training data covers S_policy!

EXPECTED RESULTS (CartPole)
---------------------------
  Expert:  200 Â± 50
  DAgger:  195 Â± 55  (97% of expert!)
  GAIL:    180 Â± 60  (90% of expert)
  BC:      150 Â± 70  (75% of expert)
  Random:   20 Â± 10

DAgger improvement over BC: ~30%
Expert queries: ~2000-3000 total

DAgger VS BC VS GAIL
--------------------
| Feature         | BC      | DAgger     | GAIL            |
|-----------------|---------|------------|-----------------|
| Training        | One-shot| Iterative  | Adversarial     |
| Expert Access   | Demos   | Queries    | Demos           |
| Distribution    | Off     | On-policy  | Dist matching   |
| Shift Robust    | Poor    | Excellent  | Good            |
| Sample Eff      | High    | Medium     | Lower           |
| Implementation  | Simplest| Moderate   | Complex         |

THEORETICAL FOUNDATION
----------------------
DAgger reduces imitation learning to no-regret online learning:
  â€¢ Each iteration reduces distribution mismatch
  â€¢ Provably converges to expert performance
  â€¢ Mistake bound: O(TâˆšN) where T=horizon, N=iterations

WHEN TO USE DAgger
------------------
âœ… Use DAgger when:
  â€¢ You have access to an expert oracle
  â€¢ Distribution shift is a problem
  â€¢ You can afford iterative training
  â€¢ Expert queries are cheaper than full demonstrations

âŒ Don't use DAgger when:
  â€¢ Expert oracle unavailable (use BC or GAIL)
  â€¢ Expert queries are very expensive
  â€¢ One-shot learning is required
  â€¢ Real-time constraints exist

================================================================================

DEMO 7: CURL CONTRASTIVE (CURL_contrastive.py)
===============================================

OVERVIEW
--------
Contrastive Unsupervised Representations for Reinforcement Learning.
Self-supervised representation learning improves RL sample efficiency.

KEY FEATURES
------------
1. Contrastive Learning Architecture
   
   â€¢ CURLEncoder: CNN encoder for visual features
   â€¢ Query Encoder: Trainable encoder (gets gradient updates)
   â€¢ Key Encoder: Momentum-updated encoder (slow-moving target)
   â€¢ Projection head for contrastive learning (128-dim latent)

2. Momentum Encoder
   - Key encoder updated via exponential moving average
   - Momentum parameter Ï„ = 0.99 (slow updates)
   - Provides stable targets for contrastive learning
   - Inspired by MoCo (Momentum Contrast)

3. Data Augmentation for Contrastive Learning
   
   RandomCropAugmentation: Creates positive pairs
   - Same observation â†’ Two different random crops
   - Positive pair: (crop1, crop2) of same observation
   - Negative pairs: Different observations in batch

4. InfoNCE Loss
   - Contrastive loss function
   - Pulls positive pairs together in embedding space
   - Pushes negative pairs apart
   - Temperature scaling (Ï„ = 0.1)
   - Implemented as cross-entropy with diagonal targets

5. CURL Module
   - Complete contrastive learning system
   - Bilinear similarity: query^T W key
   - W matrix learns optimal feature comparison
   - L2 normalization of features before comparison

6. Integration with SAC
   - CURL encoder shared between contrastive and RL tasks
   - Q-network uses CURL features
   - Actor uses CURL features
   - Decoupled representation and RL learning

CONTRASTIVE LEARNING FRAMEWORK
-------------------------------
```
Observation â†’ [Random Crop 1] â†’ Query Encoder â†’ q
           â†˜ [Random Crop 2] â†’ Key Encoder   â†’ k

Similarity: sim(q, k) = q^T W k

Loss: InfoNCE pulls (q, k) together
             pushes (q, k') apart for k' â‰  k
```

WHY IT WORKS
------------
1. Self-supervision: Learn from augmentations (no labels needed)
2. Invariance: Features ignore irrelevant transformations
3. Better features: Contrastive task creates useful representations
4. Transfer: Features help both contrastive and RL tasks

DUAL OPTIMIZATION
-----------------
```python
# CURL update (representation learning)
curl_loss = InfoNCE(query, key)
curl_optimizer.step()
momentum_update(key_encoder)

# RL update (policy learning)
q_loss = MSE(Q(s,a), target)
q_optimizer.step()
```

InfoNCE LOSS EXPLAINED
----------------------
Given batch of N observations:
  - Positive pair: (obs_i_crop1, obs_i_crop2) â†’ same obs
  - Negative pairs: (obs_i, obs_j) for i â‰  j â†’ different obs

  Logits[i,j] = similarity(query_i, key_j)

  Loss = CrossEntropy(Logits, diagonal_labels)
       = -log(exp(sim(q_i, k_i)) / Î£â±¼ exp(sim(q_i, k_j)))

MOMENTUM ENCODER UPDATE
-----------------------
```python
# Slow-moving key encoder
Î¸_key â† Ï„ Ã— Î¸_key + (1-Ï„) Ã— Î¸_query

# Ï„ = 0.99: Very slow updates
# Provides stable targets for contrastive learning
```

HYPERPARAMETERS
---------------
  CURL_LATENT_DIM = 128    # Contrastive feature dimension
  MOMENTUM = 0.99          # Key encoder momentum
  TEMPERATURE = 0.1        # InfoNCE temperature
  CURL_WEIGHT = 1.0        # Contrastive loss weight
  LEARNING_RATE = 1e-4     # All networks

VISUALIZATIONS
--------------
â€¢ Training curves (6 metrics)
â€¢ t-SNE visualization of learned representations
  - Clustering: Similar states group together
  - Separation: Different states separate
  - Structure: Meaningful organization emerges

SAMPLE EFFICIENCY GAINS
------------------------
CURL achieves better performance with same data:
  Without CURL:  150 return @ 30k steps
  With CURL:     180 return @ 30k steps
  Improvement:   20% better sample efficiency

CURL VS STANDARD RL
-------------------
| Feature          | Standard RL   | CURL              |
|------------------|---------------|-------------------|
| Representation   | Task-specific | Self-supervised   |
| Learning         | Single task   | Dual task         |
| Sample Efficiency| Baseline      | Improved          |
| Features         | May overfit   | More general      |
| Augmentation     | Optional      | Essential         |

RESEARCH IMPACT
---------------
CURL showed that:
  â€¢ Self-supervised learning helps RL
  â€¢ Contrastive methods transfer to control
  â€¢ Simple augmentations (crops) are powerful
  â€¢ Momentum encoders stabilize learning

This bridges computer vision (contrastive learning) and RL!

================================================================================

DEMO 8: RAD STRATEGIES (RAD_strategies.py)
===========================================

OVERVIEW
--------
Reinforcement Learning with Augmented Data - comprehensive comparison of
augmentation strategies for visual RL.

KEY FEATURES
------------
1. Comprehensive Augmentation Library (RADAugmentations)
   
   9+ augmentation strategies:
   
   â€¢ random_crop: Pad + random crop (MOST EFFECTIVE)
     - Provides translation invariance
     - Key technique for visual RL
   
   â€¢ random_shift: Translation augmentation
     - Similar to crop, different implementation
   
   â€¢ cutout: Random rectangular occlusion (zeros)
     - Occlusion robustness
   
   â€¢ cutout_color: Random occlusion with random color
     - More realistic occlusion
   
   â€¢ random_flip: Horizontal flip
     - Symmetry augmentation
   
   â€¢ random_rotation: Small angle rotations
     - Rotational invariance
   
   â€¢ color_jitter: Brightness + contrast variation
     - Lighting robustness
   
   â€¢ random_grayscale: Probabilistic grayscale conversion
     - Color invariance
   
   â€¢ no_augmentation: Baseline (identity)

2. Ablation Study Framework
   - Compare multiple augmentation strategies
   - Fair comparison (same hyperparameters)
   - Statistical evaluation
   - Performance ranking

3. RAD Algorithm Implementation
   - Q-learning with augmentation
   - Multiple augmented views per sample (default: 2)
   - Averages loss over augmentations
   - Target network with soft updates

VISUALIZATIONS
--------------
A. Augmentation Gallery
   - Side-by-side visualization of all strategies
   - Shows effect of each augmentation

B. Performance Comparison
   - Bar plots with error bars
   - Box plots showing distributions
   - Sorted by performance

C. Learning Curves
   - All strategies on same plot
   - Sample efficiency comparison

AUGMENTATION STRATEGIES EXPLAINED
----------------------------------

Random Crop (Most Reliable):
  Original: 84Ã—84
  Pad: +4 pixels â†’ 92Ã—92
  Crop: Random 84Ã—84 region
  Effect: Translation invariance

Cutout (Occlusion Robustness):
  Random position: (x, y)
  Mask size: 20Ã—20
  Fill: Zeros or random color
  Effect: Occlusion invariance

Color Jitter (Lighting Robustness):
  Brightness: Ã—(0.8 to 1.2)
  Contrast: Ã—(0.8 to 1.2)
  Effect: Lighting invariance

EXPECTED RESULTS (CartPole)
---------------------------
Performance Ranking (typical):
  1. Crop:         180 Â± 50  (Best)
  2. Shift:        175 Â± 55
  3. Color Jitter: 165 Â± 60
  4. Cutout:       160 Â± 65
  5. None:         150 Â± 70  (Baseline)

Improvement: 20% with best augmentation

TASK-SPECIFIC INSIGHTS
----------------------
| Task Type      | Best Augmentations  | Avoid          |
|----------------|---------------------|----------------|
| Navigation     | Crop, Shift         | Flip, Rotate   |
| Manipulation   | Crop, Color Jitter  | Flip           |
| Atari Games    | Crop, Cutout        | Grayscale      |
| Robotics       | Crop, Color Jitter  | Rotate         |

RAD VS DrQ VS CURL
------------------
| Method | Focus              | Augmentations | Loss           |
|--------|--------------------|---------------|----------------|
| DrQ    | Single aug (crop)  | 1 type        | Q-learning     |
| RAD    | Multiple augs      | 8+ types      | Q-learning     |
| CURL   | Contrastive learn  | Crop          | InfoNCE + QL   |

BEST PRACTICES FROM RAD
-----------------------
1. Start with crop: Most reliable across tasks
2. Avoid semantic breaks: Don't flip if left/right matters
3. Task matters: Best aug depends on task
4. Combine carefully: Multiple augs can help or hurt
5. Ablate systematically: Empirical evaluation essential

WHEN TO USE EACH AUGMENTATION
------------------------------

âœ… Always Safe:
  â€¢ Random Crop
  â€¢ Random Shift
  â€¢ Color Jitter

âš ï¸ Task-Dependent:
  â€¢ Cutout (if occlusion is realistic)
  â€¢ Flip (if left/right symmetry exists)
  â€¢ Grayscale (if color is irrelevant)

âŒ Usually Harmful:
  â€¢ Rotation (breaks spatial relationships)
  â€¢ Extreme jitter (breaks visual coherence)

RESEARCH INSIGHTS
-----------------
RAD's key findings:
  1. Crop is king: Works for almost all tasks
  2. Simplicity wins: Simple augs often beat complex
  3. Task-specific: No universal best augmentation
  4. Diminishing returns: More augs â‰  always better
  5. Stability matters: Some augs hurt training

================================================================================

DEMO 9: DOMAIN RANDOMIZATION (domain_randomization.py)
=======================================================

OVERVIEW
--------
Domain randomization for sim-to-real transfer. Training on diverse visual
appearances improves robustness and real-world transfer.

KEY FEATURES
------------
1. Comprehensive Domain Randomization Module (DomainRandomization)
   
   Multiple randomization techniques:
   
   â€¢ Color Randomization: Per-channel color multipliers
   â€¢ Brightness Randomization: Overall illumination changes
   â€¢ Contrast Randomization: Contrast adjustment
   â€¢ Gaussian Noise: Sensor noise simulation
   â€¢ Hue Shift: Color space rotation
   â€¢ Saturation Randomization: Color intensity variation
   â€¢ Combined Randomization: All techniques together

2. Domain Randomized Environment Wrapper
   - Applies randomization to observations
   - Wraps any Gym environment
   - Configurable randomization intensity
   - Supports both RGB and grayscale

3. Sim-to-Real Transfer Experiment
   ```
   Training: Randomized simulation
   Testing:  Canonical/"real" environment
   Metric:   Zero-shot transfer performance
   ```

4. Transfer Evaluation - Four Scenarios:
   1. Randomized â†’ Real: Main transfer test
   2. No Rand â†’ Real: Baseline
   3. Randomized â†’ Rand: Training performance
   4. No Rand â†’ Rand: Overfitting test

VISUALIZATIONS
--------------
A. Randomization Effects
   - Side-by-side: Original vs. randomized
   - 8 different randomized versions
   - Shows diversity

B. Training Comparison
   - Learning curves with/without randomization
   - Loss curves
   - Smoothed for clarity

C. Transfer Analysis
   - Bar plots with error bars
   - Box plots showing distributions
   - All four transfer scenarios

CORE INSIGHT
------------
```
Train on diverse simulations â†’ Robust to variations â†’ Better real-world transfer

Diversity during training = Robustness at test time
```

RANDOMIZATION TECHNIQUES
------------------------
1. Colors:      RGB channels Ã— random multipliers
2. Brightness:  Intensity Ã— random factor
3. Contrast:    (pixel - mean) Ã— random factor + mean
4. Noise:       Add Gaussian noise
5. Hue:         Rotate color channels
6. Saturation:  Blend with grayscale

RANDOMIZATION INTENSITY
-----------------------
  Low (0.1):    Subtle variations
  Medium (0.2): Moderate diversity (RECOMMENDED)
  High (0.3):   Strong randomization
  Extreme (0.5): May harm learning

SIM-TO-REAL GAP
---------------

Without Domain Randomization:
  Simulation: Perfect rendering, consistent visuals
  Real World: Varying lighting, textures, noise
  Result:     Policy fails due to visual mismatch

With Domain Randomization:
  Simulation: Random lighting, textures, noise
  Real World: Just another random variation!
  Result:     Policy generalizes successfully

BEST PRACTICES
--------------

âœ… Do Randomize:
  â€¢ Lighting conditions
  â€¢ Colors and textures
  â€¢ Camera angles (if variable)
  â€¢ Sensor noise
  â€¢ Background elements

âŒ Don't Randomize:
  â€¢ Task-critical features (e.g., object shape for grasping)
  â€¢ Physics parameters (handle separately)
  â€¢ Reward-relevant information
  â€¢ Spatial relationships (usually)

RANDOMIZATION STRATEGIES
------------------------

Conservative (Low variance):
  color_intensity = 0.1
  brightness_intensity = 0.1
  noise_level = 3.0

Moderate (Recommended):
  color_intensity = 0.2
  brightness_intensity = 0.2
  noise_level = 5.0

Aggressive (High variance):
  color_intensity = 0.3
  brightness_intensity = 0.3
  noise_level = 10.0

EVALUATION PROTOCOL
-------------------
1. Train on randomized simulation
2. Freeze policy
3. Test on canonical environment (no randomization)
4. Measure: Zero-shot transfer performance
5. Compare: vs. policy trained without randomization

KEY METRICS
-----------

Robustness:
  min(performance_on_canonical, performance_on_randomized)

Transfer Gap:
  performance_real - performance_sim

Generalization:
  performance_on_unseen_variations

PRACTICAL APPLICATIONS
----------------------

Robotics:
  â€¢ Train in Isaac Gym, deploy on real robot
  â€¢ Randomize: lighting, textures, camera noise
  â€¢ Result: Sim-to-real transfer without real data

Autonomous Driving:
  â€¢ Train in CARLA simulator
  â€¢ Randomize: weather, lighting, traffic
  â€¢ Result: Robust to real-world conditions

Manufacturing:
  â€¢ Train with varied object appearances
  â€¢ Randomize: colors, positions, orientations
  â€¢ Result: Generalize to new products

RESEARCH IMPACT
---------------
Domain randomization showed that:
  â€¢ Diversity during training â†’ Robustness at test
  â€¢ Sim-to-real transfer is possible without real data
  â€¢ Simple randomization beats complex domain adaptation
  â€¢ Works across vision, robotics, control tasks

LIMITATIONS
-----------
âš ï¸ Reality Gap: Some aspects hard to randomize
âš ï¸ Sample Efficiency: More diversity = slower learning
âš ï¸ Hyperparameters: Intensity needs tuning
âš ï¸ Physical Plausibility: Random â‰  realistic

================================================================================

DEMO 10: RT-1 ARCHITECTURE (RT1_architecture_demo.py)
======================================================

ðŸŽ“ CAPSTONE DEMO - FOUNDATION MODEL CONCEPTS

OVERVIEW
--------
Demonstrates vision-language-action transformer architecture inspired by RT-1
(Robotics Transformer). Shows how foundation models work for embodied AI.

KEY FEATURES
------------

1. VISION TOKENIZATION (ImageTokenizer)
   - Patch-based tokenization: Splits 84Ã—84 image into patches
   - Patch size: 14Ã—14 â†’ 36 patches total
   - Convolutional patch embedding: Efficient extraction
   - Positional embeddings: Learnable position encoding
   - Output: Sequence of vision tokens [batch, 36, 256]

2. LANGUAGE INSTRUCTION ENCODING (LanguageEncoder)
   - Token embeddings: Maps words to vectors
   - Positional encoding: Sequence position information
   - Transformer encoder: 2-layer processing
   - Attention masking: Handles variable-length instructions
   - Output: Encoded instruction tokens [batch, seq_len, 256]

3. RT-1 TRANSFORMER (RT1Transformer)
   - Multimodal token fusion: Concatenates vision + language
   - Action query token: Special learnable token for action
   - Self-attention layers: 4-layer transformer encoder
   - Cross-modal attention: Vision â†” Language interaction
   - Action prediction head: Maps action token â†’ action logits

4. COMPLETE RT-1 POLICY
   ```
   Image (84Ã—84Ã—3) â†’ Vision Tokens (36Ã—256)
                      â†“
   Instruction     â†’ Language Tokens (20Ã—256)
                      â†“
                 [Vision | Language | Action Query]
                      â†“
                 Transformer Encoder (4 layers)
                      â†“
                 Action Token â†’ Action Head
                      â†“
                 Action Logits
   ```

5. MULTI-TASK LEARNING
   - Multiple instructions: "balance pole", "keep stable", etc.
   - Shared policy: Same network for all tasks
   - Instruction conditioning: Task specified via language
   - Foundation model principle: One model, many tasks

RT-1 ARCHITECTURAL FLOW
------------------------
```
TOKENIZATION EVERYTHING:

Images  â†’ Patches  â†’ Tokens
Text    â†’ Words    â†’ Tokens
Actions â†’ Discrete â†’ Tokens

Everything is a token sequence!
```

TRANSFORMER ARCHITECTURE
------------------------
â€¢ Self-attention: All tokens attend to all tokens
â€¢ Multimodal: Vision and language in same space
â€¢ Scalable: Grows with data and compute
â€¢ Flexible: Easy to add new modalities

FOUNDATION MODEL PRINCIPLES
----------------------------
1. Large-scale pretraining: Millions of demonstrations
2. Multi-task learning: Hundreds of tasks
3. Transfer learning: Generalize to new tasks
4. Instruction following: Language-conditioned control

VISUALIZATIONS
--------------

A. Architecture Diagram (7 components):
   1. Visual Input
   2. Image Tokenization
   3. Language Encoding
   4. Token Concatenation
   5. Transformer Processing
   6. Action Token Extraction
   7. Action Prediction

B. Tokenization Process:
   â€¢ Original image display
   â€¢ Patch grid overlay
   â€¢ Token embedding heatmap
   â€¢ Shows how images become sequences

C. Training Curves:
   â€¢ Episode returns over time
   â€¢ Loss convergence
   â€¢ Smoothed for clarity

MODEL STATISTICS
----------------
Demo Implementation:
  - Vision Tokenizer:     ~100K parameters
  - Language Encoder:     ~500K parameters  
  - RT-1 Transformer:     ~2M parameters
  - Action Head:          ~100K parameters
  Total:                  ~2.7M parameters

Real World:
  - Real RT-1:            ~35M parameters
  - Real RT-2:            ~55B parameters (with PaLI)

RT-1 â†’ RT-2 EVOLUTION
---------------------
| Feature        | RT-1              | RT-2                    |
|----------------|-------------------|-------------------------|
| Vision         | Custom CNN/ViT    | PaLI (pretrained)       |
| Language       | Task-specific     | Internet-scale LLM      |
| Training       | Robotics only     | Vision-language + robot |
| Parameters     | 35M               | 55B                     |
| Reasoning      | Limited           | Chain-of-thought        |
| Generalization | Good              | Exceptional             |

FOUNDATION MODEL SCALING
------------------------

Data Scaling:
  RT-1:  130K trajectories, 700 tasks
  RT-2:  Internet-scale vision-language data + robotics

Model Scaling:
  Small:  ~10M parameters
  Medium: ~100M parameters  
  Large:  ~1B parameters
  XLarge: ~10B+ parameters

Emergent Capabilities:
  â€¢ Zero-shot: New tasks without training
  â€¢ Few-shot: Learn from demonstrations
  â€¢ Chain-of-thought: Reasoning about actions
  â€¢ Multimodal understanding: Rich perception

WHY TRANSFORMERS FOR ROBOTICS?
-------------------------------
1. Sequence modeling: Natural for time-series control
2. Attention: Focus on relevant visual/language features
3. Scalability: Proven to scale to billions of parameters
4. Transfer: Pretrained models accelerate learning
5. Multimodal: Unified architecture for vision + language

WHY FOUNDATION MODELS?
----------------------
1. Data efficiency: Leverage internet-scale pretraining
2. Generalization: Transfer across tasks and embodiments
3. Reasoning: LLM capabilities for planning
4. Open-ended: Handle novel situations
5. Democratization: Shared models benefit all

THE PATH FORWARD
----------------
```
Specialized RL â†’ Multi-task RL â†’ Foundation Models
Small models  â†’ Large models   â†’ Internet-scale
Single robot  â†’ Many robots    â†’ Universal policies
```

THE FUTURE: FOUNDATION MODELS FOR ROBOTICS
-------------------------------------------
â€¢ Scaling: Bigger models, more data, more compute
â€¢ Generalization: Cross-task, cross-embodiment
â€¢ Reasoning: LLM integration for planning
â€¢ Sim-to-real: Robust transfer via scale
â€¢ Open research: Open X-Embodiment datasets

================================================================================

COMPLETE DEMO SERIES SUMMARY
=============================

ðŸŽ‰ CONGRATULATIONS! You have completed all 10 comprehensive demos!

PROGRESSIVE LEARNING ARC
------------------------

Part 1: Core Visual RL Methods (Demos 1-4)
  âœ“ Behavioral Cloning - Imitation learning basics
  âœ“ PPO - Policy gradients for visual control
  âœ“ SAC - Off-policy continuous control
  âœ“ DrQ - Data augmentation (crop focus)

Part 2: Advanced Imitation Learning (Demos 5-6)
  âœ“ GAIL - Adversarial imitation
  âœ“ DAgger - Interactive imitation

Part 3: Representation Learning & Robustness (Demos 7-9)
  âœ“ CURL - Contrastive representation learning
  âœ“ RAD - Augmentation strategy comparison
  âœ“ Domain Randomization - Sim-to-real transfer

Part 4: Foundation Models (Demo 10)
  âœ“ RT-1 - Foundation model architecture

DEMO CHARACTERISTICS
--------------------
Each demo is:
  ðŸŽ“ Educational: Clear explanations and comments
  ðŸ”¬ Self-contained: Runs independently
  ðŸ“Š Visual: Comprehensive plots and analysis
  ðŸ—ï¸ Well-structured: Clean, modular code
  ðŸ“š Referenced: Cites original papers

COMMON PATTERNS ACROSS DEMOS
-----------------------------

1. Environment Wrappers
   - VisualWrapper: Converts any Gym env to pixel observations
   - Frame stacking: Temporal information (4 frames)
   - Grayscale conversion: Reduces input channels

2. CNN Architectures
   - Nature DQN architecture (standard)
   - 32â†’64â†’64 filters
   - 8Ã—4, 4Ã—2, 3Ã—1 kernel sizes
   - Xavier/orthogonal initialization

3. Training Patterns
   - Replay buffers (off-policy methods)
   - Rollout buffers (on-policy methods)
   - Gradient clipping (stability)
   - Learning rate scheduling
   - Early stopping / convergence criteria

4. Evaluation Protocols
   - Multiple evaluation episodes (10-50)
   - Statistical analysis (mean Â± std)
   - Comparison with baselines
   - Distribution analysis

5. Visualization Standards
   - Training curves (smoothed)
   - Performance comparisons (bar plots, box plots)
   - Method-specific visualizations
   - High-quality figures (matplotlib/seaborn)

KEY TAKEAWAYS BY TOPIC
----------------------

Imitation Learning:
  â€¢ BC: Simple but suffers from distribution shift
  â€¢ GAIL: Distribution matching via adversarial training
  â€¢ DAgger: Interactive queries fix distribution shift
  â€¢ Lesson: Distribution matching > action matching

Visual RL:
  â€¢ PPO: Stable on-policy learning
  â€¢ SAC: Sample-efficient off-policy learning
  â€¢ Lesson: Off-policy often better for pixels

Data Augmentation:
  â€¢ DrQ: Random crop is remarkably effective
  â€¢ RAD: Task-specific augmentation selection matters
  â€¢ CURL: Contrastive learning + augmentation
  â€¢ Lesson: Always use random crop for visual RL!

Robustness:
  â€¢ Domain Randomization: Sim-to-real via diversity
  â€¢ Lesson: Training diversity â†’ test robustness

Foundation Models:
  â€¢ RT-1: Transformer architecture for robotics
  â€¢ Lesson: Scale + multimodal = generalization

PERFORMANCE HIERARCHY (CartPole Visual Control)
-----------------------------------------------
  Expert:           200 Â± 50
  DAgger:           195 Â± 55  (97% of expert)
  DrQ/CURL:         180 Â± 60  (90% of expert)
  GAIL:             180 Â± 60  (90% of expert)
  PPO/SAC:          170 Â± 65  (85% of expert)
  BC:               150 Â± 70  (75% of expert)
  Random:            20 Â± 10

COMPUTATIONAL REQUIREMENTS
--------------------------
All demos designed to run on:
  â€¢ CPU: Works (slower)
  â€¢ Single GPU: Recommended
  â€¢ Time per demo: 10-30 minutes (depending on settings)

HYPERPARAMETER GUIDELINES
--------------------------

Learning Rates:
  â€¢ Policy networks: 1e-4 to 3e-4
  â€¢ Q-networks: 1e-4 to 1e-3
  â€¢ Discriminators (GAIL): 3e-4

Batch Sizes:
  â€¢ On-policy (PPO): 64-256
  â€¢ Off-policy (SAC, DrQ): 128-256

Exploration:
  â€¢ Epsilon decay: 0.995
  â€¢ Entropy coefficients: 0.01-0.1
  â€¢ Temperature (SAC): Auto-tuned

Network Architectures:
  â€¢ CNN features: 256-512 dimensions
  â€¢ MLP hidden: 128-256 dimensions
  â€¢ Transformer: 256-512 embedding dim

RECOMMENDED LEARNING PATH
--------------------------

For Students:
  1. Start with BC (simplest)
  2. Move to PPO (core RL)
  3. Compare SAC (off-policy)
  4. Add DrQ (augmentation)
  5. Try GAIL or DAgger (advanced imitation)
  6. Explore CURL/RAD (representation learning)
  7. Understand Domain Rand (robustness)
  8. Study RT-1 (foundation models)

For Researchers:
  â€¢ Focus on demos relevant to your work
  â€¢ Modify architectures for your domain
  â€¢ Combine techniques (e.g., CURL + DrQ + Domain Rand)
  â€¢ Scale up for real problems

EXTENDING THE DEMOS
--------------------

Easy Extensions:
  â€¢ Try different environments (Atari, MuJoCo)
  â€¢ Modify network architectures
  â€¢ Adjust hyperparameters
  â€¢ Combine techniques

Advanced Extensions:
  â€¢ Multi-task learning across environments
  â€¢ Continuous action spaces
  â€¢ Hierarchical policies
  â€¢ Meta-learning / few-shot adaptation
  â€¢ Real robot deployment

TROUBLESHOOTING
---------------

Common Issues:
  1. Slow learning: Reduce learning rate
  2. Instability: Add gradient clipping, reduce LR
  3. Poor performance: Check CNN architecture, augmentation
  4. Memory errors: Reduce batch size, buffer size
  5. GPU errors: Check CUDA compatibility

Debugging Tips:
  â€¢ Start with small number of episodes
  â€¢ Visualize observations (check preprocessing)
  â€¢ Monitor gradient norms
  â€¢ Compare with random policy
  â€¢ Check data distribution

CITATION & REFERENCES
---------------------

If using these demos in research or teaching, please reference:

  Lecture 13: Visual Policy Learning - From Imitation to Foundation Models
  Prof. David Olivieri
  Universidad de Vigo
  Artificial Vision Course (VIAR25/26)

Original Paper References:
  â€¢ BC: General supervised learning
  â€¢ PPO: Schulman et al., "Proximal Policy Optimization" (2017)
  â€¢ SAC: Haarnoja et al., "Soft Actor-Critic" (2018)
  â€¢ DrQ: Kostrikov et al., "Image Augmentation Is All You Need" (2020)
  â€¢ GAIL: Ho & Ermon, "Generative Adversarial Imitation Learning" (2016)
  â€¢ DAgger: Ross et al., "A Reduction of Imitation Learning..." (2011)
  â€¢ CURL: Srinivas et al., "CURL: Contrastive Unsupervised..." (2020)
  â€¢ RAD: Laskin et al., "Reinforcement Learning with Augmented Data" (2020)
  â€¢ Domain Rand: Tobin et al., "Domain Randomization..." (2017)
  â€¢ RT-1: Brohan et al., "RT-1: Robotics Transformer..." (2022)

ADDITIONAL RESOURCES
--------------------

Recommended Reading:
  â€¢ Sutton & Barto: "Reinforcement Learning: An Introduction"
  â€¢ Goodfellow et al.: "Deep Learning"
  â€¢ OpenAI Spinning Up: spinningup.openai.com
  â€¢ Lilian Weng's Blog: lilianweng.github.io

Frameworks & Libraries:
  â€¢ Stable-Baselines3: github.com/DLR-RM/stable-baselines3
  â€¢ CleanRL: github.com/vwxyzjn/cleanrl
  â€¢ RLlib: docs.ray.io/en/latest/rllib
  â€¢ Tianshou: github.com/thu-ml/tianshou

Datasets:
  â€¢ Open X-Embodiment: robotics-transformer-x.github.io
  â€¢ Atari 2600: github.com/openai/gym
  â€¢ DMControl: github.com/deepmind/dm_control

ACKNOWLEDGMENTS
---------------

These demonstrations were created for educational purposes to help students
understand the progression from classical imitation learning to modern
foundation models for embodied AI.

Special thanks to the research community for developing these methods and
making their work accessible through open publications.

================================================================================

GETTING STARTED
===============

To run any demo:

1. Install dependencies:
   pip install torch numpy gymnasium matplotlib seaborn scikit-learn

2. Optional (for some demos):
   pip install opencv-python

3. Run a demo:
   python behavioral_cloning.py
   python PPO_visual_control.py
   python SAC_pixels.py
   ... (etc)

4. Adjust settings:
   - Edit configuration section in each demo
   - Modify NUM_EPISODES, BATCH_SIZE, etc.
   - Change environment (ENV_NAME)

5. Experiment:
   - Try different hyperparameters
   - Compare different methods
   - Visualize results
   - Build on the code

================================================================================

FINAL NOTES
===========

These demos prioritize educational clarity over production performance.
For real research/applications:
  â€¢ Scale up model sizes
  â€¢ Use more training data
  â€¢ Tune hyperparameters carefully
  â€¢ Consider domain-specific modifications
  â€¢ Validate on diverse environments

The field of visual policy learning is rapidly evolving. These demos capture
the state of the art as of 2022-2023, with RT-1/RT-2 representing the frontier.
New methods continue to emerge!

We hope these demonstrations help you understand and implement modern visual
policy learning methods. Happy learning and researching!

================================================================================

Questions or Issues?
Contact: Prof. David Olivieri
Universidad de Vigo, Spain

================================================================================
END OF README
================================================================================